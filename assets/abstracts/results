1501-Delinquent-Debt-Repayment.txt:Agency Objective. Increase the rate of debt recovery by simplifying notification letters. Background. The Department of the Treasury’s Debt Management Services (DMS) collects delinquent debts from individuals who have incurred and fallen behind on non-tax debts to Federal agencies.65 Agencies are required to refer debts to DMS at 180 days delinquent.66 DMS collection letters are typically sent within 4 days of receiving the debt from a referring Federal agency. In 2013, DMS, in collaboration with the Office of Evaluation Sciences (OES), modified its collection letter using behavioral insights to increase the rate of debt recovery. Methods. The new letter was simplified (e.g., multiple mailing addresses linked to the Department of Treasury were removed from the letter and replaced with a single address; the web address for online payment was shortened substantially), personalized (individuals were addressed by name rather than by debt ID number), and the total debt owed was emphasized in the letter’s opening line. In addition, based on recent research from the United Kingdom showing that social comparisons can increase tax collections, the new letter highlighted the fact that 91 percent of Americans pay their debt on time.67 To determine the effectiveness of the changes, the new letter was compared to the status quo delinquency letter during a trial conducted between October 2013 and March 2014. Individuals (n = 21,305) with debts from six creditor agencies were randomly assigned to be sent either the status quo or new letter. Three outcomes were measured at 30 and 60 days from sending: (i) receipt of payment on debts; (ii) debtor contact via inbound calls to DMS; and, (iii) debtor-initiated payments to pay.gov. Results. No differences were found in overall payment rates or inbound calls across the letter types. Individuals were significantly more likely to make a payment via pay.gov (rather than by mail) if they were sent the new letter (2.16 versus 1.49 percent, p < 0.05, 95% CI [0.30, 1.04]). Some collection was made within 30 days on 5.89 percent of all debts included in the sample. There were significant differences across agencies in the proportion of all debts on which payment was received (ranging from 1.79 to 12.78 percent). Debt size was also a significant predictor of payment (smaller debts were more likely to receive payment). Conclusions. The evidence from the trial suggests that the new letter did not affect overall payment rates on delinquent, non-tax debt. It did affect the method of payment, however, by increasing utilization of online payments.
1502-Servicemember-Roth-TSP-Re-Enrollment.txt:Agency Objective. Increase re-enrollments in Roth Thrift Savings Plans among interested Servicemembers, using action-oriented, simplified email communications. Background. Due to a change in the military pay system, 139,273 members of the Armed Forces needed to re-enroll in their Roth Thrift Savings Plans (TSP) in January 2015 to avoid having their contributions suspended indefinitely. To re-enroll, Servicemembers needed to log in to the Department of Defense’s (DOD) Defense Finance and Accounting Service (DFAS) MyPay website and select a contribution percentage. DFAS conducted a broad media and email campaign to encourage affected Servicemembers to re-enroll in TSP. Methods. As part of this campaign, DFAS, in collaboration with the Office of Evaluation Sciences (OES), redesigned the standard email sent by DFAS to Servicemembers announcing the opening of the reenrollment window. The redesigned, simplified email emphasized the New Year as a fresh start, laid out the three steps needed to complete the re-enrollment process, and encouraged action in order to avoid losing the chance to contribute savings.46 The 139,273 Servicemembers were assigned to two groups based on the last two digits of their Social Security Number (SSN), with those with SSNs ending in 0–49 (n = 69,318) sent the standard email, and Servicemembers with SSNs ending in 50–99 (n = 69,955) sent the redesigned email. After sending the emails on January 2, 2015, DFAS tracked re-enrollment requests by SSN. Results. One week after the emails were sent, 16,291 Servicemembers who were sent the standard email (23.5 percent) had re-enrolled, compared with 20,061 Servicemembers who were sent the redesigned email (28.7 percent). Because the last four digits of SSNs are effectively randomly assigned, the increase in reenrollment rates of 5.2 percentage points (p < 0.01, 95% CI [4.7, 5.6]) can be attributed to the redesigned email. This means that 22 percent more Servicemembers—3,770—re-enrolled in their Roth TSP, or at least accelerated their re-enrollment, as a result of being sent the redesigned email instead of the standard email.
1503-Income-Driven-Repayment.txt:Agency Objective. Increase applications for incomedriven repayment plans among delinquent student loan borrowers with email notices designed using behavioral insights. Background. As student loan balances have risen in recent years, an increasing numbers of borrowers have struggled to stay on track with their payments.52 At-risk and delinquent borrowers may benefit from incomedriven repayment (IDR) options, which link monthly payment amounts to their incomes, and can make repayment more manageable.53 However, enrolling in an IDR plan requires that borrowers learn of and apply for such a plan. In late 2013, the office of Federal Student Aid (FSA) within the Department of Education (ED), in collaboration with the Office of Evaluation Sciences (OES), conducted an email campaign to help at-risk student loan borrowers learn of and apply for these alternative payment plans. Methods. Borrowers who were 90 to 180 days delinquent (n = 841,442) were randomly assigned along two, independent dimensions. First, the timing of the email was varied so that half of the sample was sent the email in November and half in December 2013. Second, the format and content of the email were varied, comparing four variants designed using behavioral insights: a longer, more comprehensive email; a shorter and lessdetailed version of the email; a loss-framed email; and a gain-framed email.54 Results. Emails had a significant, positive impact on completed IDR applications. Of those borrowers who were sent an email, 1.02 percent submitted an IDR application in the 20 days following the email, compared with only 0.23 percent of those not sent the email, a difference of 0.79 percentage point (p < 0.01, 95% CI [0.75, 0.82]). Across variants, the longer email was slightly more effective at prompting IDR applications. Borrowers sent the longer email completed applications at a rate of 1.04 percent in the 20 days after sending, compared with 0.86 percent of those sent the shorter email (p < 0.01, 95% CI [0.12, 0.24]). The loss-framed email was slightly more effective at leading to applications than the gain-framed email, but each was less effective than the more comprehensive, longer email (all differences p < 0.05). Conclusions. These findings suggest that low-cost, timely notices can make a significant difference on IDR enrollment rates among struggling student loan borrowers. While the effects are small in size in relative terms, this outcome is a measure of application rates within a twenty-day window following a single email. Given the large population of at-risk and delinquent borrowers, the absolute effect is substantial: the results suggest that sending just a single email led roughly 6,600 additional borrowers to sign up for an IDR plan.
1504-Federal-Health-Insurance-Marketplace-Enrollment.txt:Agency Objective. Assist uninsured Americans with completing their health insurance application in time by sending behaviorally designed letters prior to the deadline for open enrollment. Background. During the Open Enrollment Period, qualifying individuals and families can purchase health insurance plans through the Federal Health Insurance Marketplace (FHIM).61 For the 2015 enrollment season, the close of open enrollment was February 15, 2015. As of early February 2015, many people had visited HealthCare.gov and started an online account, but not yet selected a plan. The Department of Health and Human Services (HHS), in collaboration with the Office of Evaluation Sciences (OES), developed and sent letters to assist these individuals with completing their insurance application in time. Methods. From February 2 to 4, 2015, individuals who had registered for a HealthCare.gov user account but not yet enrolled in an insurance plan (n = 811,795) were randomly assigned to be sent one of eight letter variants or no letter at all (the “hold-out” group). The eight letters varied behavioral dynamics including action language, an implementation intention prompt, a picture, social norm messaging, a pledge, and loss aversion.62 The core content of the letter about how to enroll (information about the benefits of enrolling, the deadline, the website link, and a phone number) was held constant across each of the eight letters. Results. By the February 15 deadline, enrollments were 4.03 percent in the hold-out group and 4.32 percent across letter variants—a 7.15 percent increase (p < 0.001, 95% CI [5.89, 8.42]) amounting to 1,924 marginal enrollments. Not all letters had equal effects (see figure): the highest performing letter (#4, designed with behavioral dynamics of action language, an implementation intention, and a picture) boosted enrollments by 13.17 percent compared with only 1.84 percent for letter #8 (a “kitchen sink” variant including all dynamics minus the pledge). A social norm message (about the “millions of Americans” enrolled) in #8, 6, and 5 was ineffective.
1504-Federal-Health-Insurance-Marketplace-Enrollment.txt:4 2 3 7 Letter # 1 5 6 8 0 3 4 5 Percent Enrolled (95% CI) 6
1504-Federal-Health-Insurance-Marketplace-Enrollment.txt:Enrollment rates of the eight letter variants, sorted by effectiveness. The light blue bar indicates the 95% confidence interval of the group that was sent no letter.
1505-Double-Sided-Printing.txt:Agency Objective. Reduce cost and total paper use at a Federal Government agency by employing a simple, low-cost pop-up message, delivered when users print, to discourage single-sided printing. Background. Not surprisingly given its scale, the Federal Government uses a great deal of paper. A 2009 survey indicated that the average government employee printed about 30 pages per day.74 At 2.6 million executive branch employees and 240 working days per year, that amounts to over 18 billion pages printed per year.75 Reliable figures on the rate of duplex printing (doublesided printing) across the government do not exist, but in data for this project the baseline rate of duplex printing was 46 percent of all print jobs, implying significant scope to reduce total paper use and reduce costs through the increased use of double-sided printing. Methods. The Department of Agriculture’s (USDA) Economic Research Service (ERS), in collaboration with the Office of Evaluation Sciences (OES), tested the effectiveness of adding a small cost to printing single- rather than double-sided for its employees. Behavioral science research from other contexts suggests that even relatively minor costs associated with taking one action can be sufficient to lead individuals to take a different action.76 Implementation of this cost was randomized at the printer level within ERS. The small cost to printing single-sided was introduced by presenting individuals, when they attempted to print a single-sided document at a network printer in the treatment group, with a pop-up box (an image of which is displayed at right). This pop-up required a second mouse click before single-sided printing would occur. If the individual did not click “Print” after five minutes, the print job was deleted. The text of the pop-up notified individuals that if they changed their default settings to duplex, they would not face the pop-up in the future. Results. This simple prompt increased the likelihood of duplex printing on a given job by 5.8 percentage points (p < 0.01, 95% CI [4.2, 7.4]), from a baseline of 46.0 percent.
1506-On-Base-Servicemember-TSP-Enrollment.txt:Agency Objective. Actively presenting Servicemembers with a choice to enroll in Thrift Savings Plans (TSP), in order to promote enrollment. Background. The Federal Government, including the military, operates a savings program for its employees known as the Thrift Savings Plan (TSP). Roughly 58 percent of the over 1.3 million active duty Servicemembers in the Armed Forces are not currently enrolled in any TSP plan, and only around 1 percent of non-enrolled Servicemembers newly enroll each month. The success of “prompted choice” interventions in other workplace savings contexts—where employees have to actively choose whether to contribute or not—suggests that many Servicemembers might enroll if prompted.* Because Permanent Change of Duty Station (PCS), or transferring to a new location, is often tied to changes in compensation and duties, in-processing briefings are a natural reset moment during which Servicemembers may benefit from prompts to make a financial decision. Methods. In collaboration with the Department of Defense, the Office of Evaluation Sciences (OES), designed a cover sheet for the TSP-U-1 Thrift Savings Plan Election Form that (a) provided information on benefits of TSP investing, and (b) actively prompted Servicemembers to contribute—or not—to TSP. The cover sheet was included in the in-processing package at Ft. Myer in Arlington, VA, and briefed Servicemembers were required to submit the completed cover sheet along with other required paperwork. Briefing logistics made random assignment infeasible; for purposes of estimating impacts of the treatment period at Ft. Myer, TSP enrollment data was collected on enrollment rates at Ft. Myer over the year preceding the pilot period and from a comparison set of comparable forts: Fts. Belvoir, Bragg, and Meade. Results. TSP enrollment rates for Servicemembers inprocessed and not already enrolled at Ft. Myer during the pilot period were 8.71 percent, compared to 2.91 percent in the other three forts (Belvoir, Bragg, and Meade) during the same period. During the same date range one year prior to the pilot in 2014 (4/20/14– 6/12/14), and over a comparable length window 53 days prior to the pilot period (2/23/15–4/17/15), the average enrollment rate at Ft. Myer was 4.34 percent compared with an average of 2.88 percent at the other three forts. Using a difference-in-difference estimate, the pilot is estimated to have led to a 4.3 percentage point increase in the rate of TSP enrollment among unenrolled Servicemembers (p < 0.05, 95% CI [0.26, 8.42]).
1507-Servicemember-TSP-Enrollment.txt:Agency Objective. Increase enrollment in Thrift Savings Plans among interested active duty Servicemembers using behaviorally designed email communications. Background. The Federal Government operates a workplace savings program called the Thrift Savings Plan (TSP) for all of its employees.41 While the Federal Government automatically enrolls its civilian employees in TSP, it does not automatically enroll Servicemembers, and military enrollment rates average roughly 42 percent.42 In order to enroll, Servicemembers need to log in to the Department of Defense’s (DOD) Defense Finance and Accounting Service (DFAS) MyPay website and select a contribution percentage. As TSP savings through traditional (pre-tax) or Roth (after-tax) contributions can confer both short- and long-term benefits, many of the over 800,000 unenrolled Servicemembers might choose to enroll in TSP if actively presented with a chance to do so. Methods. DFAS, in collaboration with the Office of Evaluation Sciences (OES) and academic researchers, tested the impact on TSP enrollment of sending unenrolled Servicemembers emails that highlighted the opportunity and potential benefits of TSP.43 In addition to a no-email control and a standard message drawn from TSP and DFAS web materials, eight different messages utilized different combinations of behavioral insights: clear action steps, fresh-start messaging, presentation of the decision to enroll in TSP as a choice between two options, and emphasis on short- and long-term benefits.44 The 806,861 Servicemembers who were not enrolled in TSP as of April 27 were assigned to these ten groups based on the last two digits of their Social Security Number (SSN). After DFAS sent out the emails on April 29, 2015 (May 4 for Marines), it tracked TSP enrollment by SSN. Results. One month after the emails were sent, 14,491 Servicemembers had enrolled: 920 in the noemail group (1.14 percent), compared with 1,255 sent a standard message (1.56 percent) and 12,316 across those sent the eight treatment emails (1.91 percent). The most effective email clarified the action steps needed to enroll and emphasized the potential longterm benefits of saving even a little each month. The increase in enrollment rates due to the standard email was 0.42 percentage point (p < 0.01, 95% CI [0.30, 0.53]), compared with 0.77 percentage point for treatment emails (p < 0.01, 95% CI [0.69, 0.85]). This means that 67 percent more Servicemembers—4,930—enrolled in TSP in May 2015 as a result of being sent a treatment message.
1508-Microloans-for-Farmers.txt:Agency Objective. Provide information on the USDA Microloans program to farmers. Background. The U.S. Department of Agriculture’s, (USDA) Farm Service Agency (FSA) launched the Microloan program in January 2013 to better serve the needs of small farms, beginning farmers, and farmers from historically socially disadvantaged groups.60 As their name suggests, these loans are smaller than other direct operating loans, with an initial maximum of $35,000 that was raised to $50,000 in November 2014. The loans are designed to be more convenient and accessible to nontraditional producers who might lack the business and credit history that traditional clients of the direct operating loan program have. This includes a streamlined application process and relaxed requirements for collateral and previous experience in farming. Methods. In collaboration with the Farm Service Agency (FSA), the USDA’s Economic Research Service and the Office of Evaluation Sciences (OES) designed an outreach letter that provided information on the benefits of the Microloan program, personalized contact information for local loan officers, and a shortened web address for accessing more information.* The letters were sent in late April to farmers in a random sample of zip codes in nine states: Alabama, Arkansas, Georgia, Kentucky, Louisiana, Mississippi, North Carolina, South Carolina, and Tennessee. Farmers who were interested in Microloans were encouraged to follow up by calling, emailing, or visiting their local county FSA offices to discuss options with an FSA loan officer. Letters were sent to all farms in 1,848 treatment zip codes. Results. The letter increased the number of successful applicants for Microloans. The number of farms that apply for and receive a Microloan is small compared to the overall population of farms—131 farms in control zip codes, or approximately 0.09 percent, applied for and received a Microloan between late April and July 2015. In the treatment group, that figure increased to 165 farms, or 0.11 percent of all farms in treatment zip codes, a difference of 0.02 percentage point (p < 0.05, 95% CI [0.00, 0.05]). Consistent with this result, data collected by FSA on activity at county offices indicate that the letters increased the amount of Microloan-related activity in FSA county offices. Approximately 1.8 percent of the recorded activity in FSA county offices from late April through July of 2015 had to do with Microloans. Of those customers who gave information about themselves when they inquired about Microloans, 2.85 percent of the office activity was generated by Microloans in zip codes that did not receive letters, while 4.90 percent of the office activity was generated by Microloans in zip codes that received letters, a difference of 2.05 percentage points (p < 0.01). Conclusion. Providing actionable information to new farmers can increase access to Microloan credit.
1509-Legacy-Treasury-Direct-Accounts.txt:Agency Objective. Determine if letters work to encourage account transfers by securities holders. Background. The Department of the Treasury’s Legacy Treasury Direct (LTD) program allowed investors to purchase marketable securities directly from the Treasury, via mail, fax, and phone.72 The program was phased out starting in 2011, before being decommissioned in late 2014. Before and during the phase out, Treasury sent several letters to LTD account holders encouraging them to transition their accounts to, among other places, TreasuryDirect, an online system opened in 2002. Methods. In an effort to move customers to the allelectronic system, the Bureau of the Fiscal Service (Fiscal Service) at the Treasury collaborated with the Office of Evaluation Sciences (OES) to develop two letters for LTD account holders. First, a Clarified Standard letter, a simplified version of the previous years’ letters. Second, a Default Appointment letter, with similar text as the first, and which also gave recipients an appointment with their Treasury representative during a low call volume time window at the Fiscal Service call center. The Default Appointment letter builds on behavioral science research finding that individuals are more likely to follow through on plans that identify specific moments of action.73 The letters were mailed to the 33,500 remaining LTD account holders over 16 business days, with each of the two letters mailed to batches of 1,000 accounts by zip code in sequential numerical order. Fiscal Service’s call center collected data on LTD account holder call-in actions, including indicators for actions taken by or indicated by the caller, such as whether the caller indicated an interest in taking any account actions, and whether the caller established an online account. Using letter identification numbers printed on each letter (e.g., 9965 or 8865 in the top right corner of the letter), the type of letter (Clarified Standard or Default Appointment) was linked to data on call-in actions, which allowed for observation of differences in call-in actions by letter type. Results. Compared with the Clarified Standard letter, the Default Appointment letter resulted in a call-in rate that was 2.37 percentage points higher (p < 0.05, 95% CI [1.69, 3.07]), an increase of 23 percent. Fewer than 5 percent of the LTD account holders who called in (across both treatment groups) said they would definitely switch to the online Treasury Direct system.
1510-Prescriber-Letters.txt:Agency Objective. Reduce inappropriate prescribing of controlled substances in Medicare Part D.68 Background. Inappropriate prescribing can threaten patient health and increase healthcare costs. A body of research shows evidence of some providers overprescribing certain pharmaceuticals, including controlled substances, benzodiazepines, and antipsychotics.69 Through its Center for Program Integrity (CPI), the Centers for Medicare and Medicaid Services (CMS) uses a variety of approaches to combat overprescribing behavior, such as proactively identifying providers suspected of inappropriate activity and pursuing legal action through law enforcement channels. Studies have shown that letters, especially those highlighting social comparisons, can motivate individuals to more carefully examine their own behavior. Simply stating that “9 out of 10 people pay their taxes on time,” for example, has been shown to substantially increase timely tax payments.70 Physicians are more likely to provide vaccinations after receiving feedback on their vaccination rates relative to peers.71 CPI, in collaboration with the Office of Evaluation Sciences (OES) and academic researchers, developed and sent letters to providers incorporating this behavioral insight to promote program integrity in Medicare Part D. Methods. Potential improper prescribers of Schedule II drugs (e.g., opioids) were identified as those who prescribed far more than their peers in the same state and medical specialty. These providers (n = 1,518) were randomly assigned to be sent a letter (in September 2014) or not. The letter depicted an individual’s prescribing rates in comparison to his or her peers, and provided information about proper prescribing practices. The letter was designed to educate providers and induce them to “self-audit” to correct potentially improper payments. The eﬀect of the letter on prescribing was tracked via Part D claims data. Results. Using data collected over the 90 days after the letter was mailed, comparisons failed to detect an eﬀect of the letter on Schedule II prescribing. Tests can reject that the letter reduced prescribing by more than 1.4 percent, but cannot reject effects smaller than that—though a reduction less than 1.4 percent could be medically and economically significant. Conclusions. The letter as designed did not exert a detectable effect. Given the low cost of letter interventions, and the fact that informative letters have been shown to work in other contexts, this finding has prompted the research team to explore alternative approaches to reaching providers as well as the design, timing, and frequency of the letters. Additional letter-based testing is currently underway.
1511-Chapter 36-Benefits-for Veterans.txt:Agency Objective. Increase the take-up of education and career counseling benefits among interested Veterans using email outreach designed using behavioral insights. Background. The Department of Veterans’ Affairs (VA) is committed to help military Servicemembers fully reintegrate back into society through education, training, and career counseling programs.55 VA’s Chapter 36 Education and Career Counseling benefit (Chapter 36) is a reintegration service provided by VA’s Vocational Rehabilitation and Education office (VR&E). Chapter 36 services include personalized counseling on education, career options, and accessing related VA benefits.56 The Chapter 36 application process is paper-based and involves multiple steps, which may create barriers to access. Methods. VR&E, in collaboration with the Office of Evaluation Sciences (OES) and academic researchers, identified email as a low cost tool to inform Veterans about Chapter 36 benefits, their eligibility, the application process, and what to expect after an application was submitted.57 Eligible Veterans with a valid email address on record were randomly assigned to one of three conditions:58 (1) Business as usual: sent no email from the VA about the benefit (n = 21,423); (2) Basic email: sent an email that explained they were eligible for free career counseling and provided a link to download the application form (n = 21,424); and (3) Earned: sent an email otherwise identical to the basic email, but with an emphasis on the fact that Veterans had earned the benefit through their service (n = 21,423).59 To measure the impact of the emails, four outcomes were tracked using existing VA data: email open rates, click rates (on the link to the application form), applications submitted, and Veterans who completed services with counseling. Results. The Earned email outperformed the Basic email on open (42.6 versus 40.0 percent, p < 0.01, 95% CI [1.6, 3.6]) and click rates (4.1 vs 3.6 percent, p < 0.03, 95% CI [0.11, 0.89]). The emails increased applications and completion of services with counseling measurably, but overall uptake rates remained low. Without the emails, rates of application remained near 0 (only one person in the Business as usual group applied for benefits during the trial period); the Basic email increased applications to 0.37 percent (p < 0.01) and the Earned email to 0.30 percent (p < 0.01). There is no meaningful difference between the two emails (p = 0.79). Conclusions. These results suggest that email can be an inexpensive and effective way to share information about Chapter 36 benefits, but that they may not lead to substantial improvements in utilization. More extensive changes may be necessary to promote access to these benefits. However, communication about VA benefits should describe the benefits as those that Veterans have earned through their service.
1512-1-Tenant-Satisfaction-Survey-Response-Calendar-Invite.txt:Agency Objective. Increase response rates in a workplace survey that directly informs Federal facility strategy. Background. The General Service Administration (GSA)’s Public Buildings Service (PBS) owns or leases over 9,600 assets, with more than 370 million square feet of space for over a million Federal employees. Each year, PBS sends Federal employees the Tenant Satisfaction Survey (TSS), the results of which help determine Federal facilities strategy. In 2014, PBS collaborated with the Office of Evaluation Sciences (OES) to simplify the survey and test different messages for launching the survey, with the goal of increasing response rates. Methods. Pilot TSS announcement emails were randomly assigned to a randomly selected sample of 29,997 employees at agencies using Federally owned or leased office space. A Business as Usual launch email was sent to 14,997 employees, with content based on the previous year’s announcement materials, emphasizing that the survey was online and saved the government paper and time. A Simplified Calendar Reminder launch email was sent to 15,000 employees, emphasizing what the email was requesting (“please take the survey today!”); it also had a clear, mobile-optimized button linking to the survey and an embedded link to a calendar invite recipients could use to set a reminder to take the survey at the official launch time. The two emails had distinct links to two identical versions of the survey, allowing tracking of survey views, starts, and completions by email group. Results. One week after sending the emails, the Simplified Calendar Reminder email resulted in a nearly 1 percentage point (p < 0.01, 95% CI [0.35, 1.5]) higher completion rate amongst people sent the email, a 13.5 percent higher relative completion rate. The Simplified Calendar Reminder also promoted survey completion conditional on starting the survey—among those who viewed and started the survey, it led to completion rates that were 3 percentage points higher (p < 0.01, CI [2.15, 4.25]).
1512-2-Tenant-Satisfaction-Survey-Response-Subject-Lines-and-Day-of-Week.txt:Agency Objective. Determine what day of week and time of day is best to send emails to increase email opens and clicks; determine what subject line content and punctuation increases opens and clicks. Background. The General Service Administration (GSA)’s Public Buildings Service (PBS) owns or leases over 9,600 assets, with more than 370 million square feet of space for over a million Federal employees.80 Each year, PBS sends Federal employees the Tenant Satisfaction Survey (TSS), the results of which help determine Federal facilities strategy. In 2014, PBS collaborated with the Office of Evaluation Sciences (OES) to simplify the survey and test different messages for launching the survey, with the goal of increasing response rates. After scaling the successful messages from the TSS launch pilot (see above, “TSS Response: Calendar Invite”), and overwhelming the survey contractor’s website with a full launch (see above, “TSS Response: Time of Day”), PBS and OES spaced out the remaining announcement population into three batches of roughly 23,000 recipients each day for seven business days. Methods. An announcement email was sent to two randomly assigned groups of approximately 11,500 Federal employees three times a day for a week—at 9:00 a.m., 12:30 p.m., and 4:00 p.m. (all send times Eastern). In addition to time-of-day and day-of-week variation, PBS and OES varied aspects of the subject line, testing paired permutations of the following against each other: “2014 Tenant Satisfaction Survey,” “2014 Tenant Satisfaction Survey – Take it Today” and “2014 Tenant Satisfaction Survey – Please Take it Today.” The emails also varied the use of an exclamation point on each of those subject lines. Data on recipient agency and location came from Office of Personnel Management (OPM) records. GovDelivery, the email service, provided data on email open and click rates. Results. Controlling for state and agency effects, the highest likelihoods of opens and clicks came on Tuesdays (1.8 percentage points higher open, 1.2 percentage points higher click) and Wednesdays (1.9 percentage points higher open, 1.0 percentage point higher click), relative to Fridays (p < 0.01). Relative to mornings, lunchtime sends had higher clicks (0.3 percentage point) and opens (0.3 percentage point) (p < 0.01). And relative to the lowest performing subject line (“2014 Tenant Satisfaction Survey – Take it Today”) adding “Please” led to 1.0 percentage point more clicks and 0.5 percentage point more opens (p < 0.01); adding an exclamation point led to 0.8 percentage point fewer opens, and 0.5 percentage point fewer clicks (p < 0.01). Conclusions. The findings from these TSS tests— that sending emails near lunchtime on Tuesdays and Wednesdays resulted in higher responses; and that saying “please” in subject lines helped while adding an exclamation point hurt—may not hold in every government or workplace email context. However, they provide meaningful evidence on responses to workplace requests. Perhaps more significantly, the methods and practices of testing different approaches to workplace email requests are broadly applicable across government.
1512-3-Tenant-Satisfaction-Survey-Response-Time-of-Day.txt:Agency Objective. Increase open and click rates in response to an email announcing a workplace survey. Background. The General Service Administration (GSA)’s Public Buildings Service (PBS) owns or leases over 9,600 assets, with more than 370 million square feet of space for over a million Federal employees.79 Each year, PBS sends Federal employees the Tenant Satisfaction Survey (TSS), the results of which help determine Federal facilities strategy. In 2014, PBS collaborated with the Office of Evaluation Sciences (OES) to simplify the survey and test different messages for launching the survey, with the goal of increasing response rates. Methods. In scaling the successful messages from the TSS launch pilot (see above, “TSS Response: Calendar Invite”), PBS spread out the full launch emails among the intended 873,755 recipients over the course of Thursday, July 10, 2014. In order to reduce burden on the survey site contractor and ensure the site operated, the same email was scheduled to be sent to randomly selected groups of 96,000 federal employees every hour over the course of the day (at 8:55 a.m., 9:55 a.m., etc.) (all send times Eastern). The response to these launch emails overloaded the survey contractor’s website, and further emails were postponed following the 1:55 p.m. email (the remaining workers were sent launch emails over the following week; see below, “TSS Response: Subject Lines and Day of Week”). Data on recipient agency and location came from Office of Personnel Management (OPM) records. GovDelivery, the email service, provided data on email open and click rates. Results. Relative to the 8:55 a.m. send, the highest email open and click rates came near noon, at the 11:55 a.m. send. Controlling for the effects of states and agency of recipients, emails sent at 11:55 a.m. had an open rate that was 2.5 percentage points higher (p < 0.01, 95% CI [2.16, 2.86]), and a click rate that was 1.5 percentage points higher (p < 0.01, 95% CI [1.19, 1.82])—that is, sending emails three hours later meant that approximately 1,500 more people read and followed through on the email request.
1513-Missed-Student-Loan-Payments.txt:Agency Objective. Help student loan borrowers who miss payments get on track with their payments using email reminders sent directly from Federal Student Aid. Background. Federal student loan borrowers who miss their initial loan payments are in danger of defaulting on their loan.50 To assist these borrowers, the Department of Education’s (ED) office of Federal Student Aid (FSA), in collaboration with the Office of Evaluation Sciences (OES), ran an email campaign to determine whether having FSA send reminder emails to borrowers—in addition to the communications from loan servicers they already receive—could help borrowers get back on track after missing their initial payments. A reminder email of this kind could particularly help those borrowers who are unaware that payments are now due or lack clarity about how to make payments. Methods. The email campaign took place in the last week of February 2015, targeting borrowers who had missed the first or first and second payment on their monthly loan (n = 149,115). The email made clear that the borrower had missed a payment, sought to clarify the borrower’s relationship with their loan servicer, and included a prominent, direct link to their servicer’s login page to help with making a payment. Evidence from other contexts suggests that low-cost reminders of this nature can help individuals to make payments.51 A link to more information about incomedriven repayment (IDR) plans was also included, to which borrowers were directed if they could not afford their payment. The project proceeded in two phases: In the first, FSA identified an effective subject line by randomly sending different subject lines to four groups of 12,500 borrowers each and comparing open rates. (The line “You missed a payment on your Federal student loan” was most effective by this measure). In the second phase, emails using the winning subject line were sent to a randomly selected set of the remaining borrowers (n = 77,115). A control group (n = 22,000) did not receive any communications from FSA, but continued to receive any ongoing communications from their loan servicers. Payment rates for all groups were tracked through the end of June 2015. Results. In the seven days after the emails were sent, 3.5 percent of borrowers who were sent an email made at least one payment compared to 2.7 percent of the control group—a difference of 0.8 percentage points (p < 0.01, 95% CI [0.6, 1.0]). This difference persisted: more than three months after the emails were sent, the fraction of those groups having made a payment had risen to 16.6 and 16.0 percent, respectively, a difference of 0.6 percentage point (p < 0.05, 95% CI [0.1, 1.1]). There were no significant differences in the number of IDR applications; however, borrowers who received emails were less likely to access FSA’s repayment calculator (p < 0.01) or to switch to deferment or forbearance on their loans (p < 0.01). Conclusions. Emails that reminded student loan borrowers about missed payments and clarified how to make a payment led to significantly, but modestly, higher payment rates for the treatment group, and these higher rates persisted throughout the follow-up period.
1514-2-iff-confirmation-prompt-update.txt:implementation of the modified interface, the  median self-reported sales amount was $445 (p <  0.05, 95% CI [87, 803]) higher for vendors signing  at the top of the form compared with those vendors  who were not required to make this confirmation.  The increase in IFF remittances in the treatment  group in just the third quarter of 2014 was $1.59  million.   Self-reported sales in subsequent quarters after  the trial was concluded (fourth quarter 2014  through third quarter 2015) were still higher for  treatment group vendors than for those in the  control group, but the difference was not  statistically significant in any quarter. Compared to  vendors in the control group, reported sales for  vendors signing at the top of the form were $357  (p=.51, 95% CI [-715, 1430]) higher in the fourth  quarter 2014, $619 (p=.31, 95% CI [-580, 1819])  higher in the first quarter 2015, $434 (p=.49, 95%  CI [-796, 1664]) higher in the second quarter 2015,  and $1,475 (p=.07, 95% CI [-135, 3085]) higher in  the third quarter 2015.    Conclusion Confirmation prompts at the  beginning of a form are a promising approach to  reducing financial self-reporting errors, especially  given the near-zero marginal cost to implement.  However, continued exposure to the confirmation  prompt does not appear to induce a persistent  reduction in errors over time. 
1514-Industrial-Funding-Fee-Reports.txt:Agency Objective. Reduce financial self-reporting errors using a redesigned data-entry form. Background. Federal vendors are required to pay a fee, called the industrial funding fee (IFF), currently set at 0.75 percent of quarterly sales on certain transactions.63 The size of the IFF payment is determined from self-reports submitted via a website (https://72a.gsa. gov/). In fiscal year 2013, the General Services Administration (GSA) collected approximately $269 million in IFF across roughly 47,000 transactions. The GSA introduced a confirmation prompt as part of the IFF data-entry form in order to reduce financial self-reporting errors. Research has shown that inserting a confirmation prompt, where the user signs his or her name confirming the accuracy of the self-reported statements, reduces self-report errors if done at the beginning of a form; prompts at the end of a form seem to have no effect.64 Methods. The randomized controlled trial was fielded during the third reporting quarter of 2014, where vendors (n = 18,477) were randomly assigned to use either: (a) the existing reporting system (control); or (b) a modified interface (treatment), redesigned to include an opening signature box confirming, “I promise that the information I am providing is true and accurate.” Administrative data on paid IFF provided the primary outcome measure. Results. The median self-reported sales amount was $445 (p < 0.05, 95% CI [87, 803]) higher for vendors signing at the top of the form compared with those vendors who were not required to make this confirmation. The increase in IFF remittances in the treatment group in just the third quarter of 2014 was $1.59 million. Conclusions. Confirmation prompts at the beginning of a form are a promising approach to reducing financial self-reporting errors, especially given the near-zero marginal cost to implement.
1515-Summer-Melt.txt:Agency Objective. Increase college enrollment rates among college-accepted high school graduates by sending them text message reminders to complete required pre-matriculation tasks over the summer. Background. Every year 20–30 percent of collegeaccepted high school graduates in urban districts fail to matriculate in college in the fall—a phenomenon known as “summer melt.”47 In the summer of 2014, the Department of Education’s (ED) office of Federal Student Aid (FSA), along with the Office of Evaluation Sciences (OES), provided technical expertise to the nonprofit organization uAspire and a team of researchers on a text messaging campaign to students and their parents.48 Text messages were personalized for each student and reminded each student of tasks she needed to complete to successfully matriculate. The text messages also provided recipients with a connection to individualized college and financial aid advising. Methods. High school students from five cities participated in the pilot (n = 4,882). Students were randomly assigned to one of three groups: (1) a control group that was not sent messages; (2) a group in which only students were sent messages; (3) a group in which both students and their parents were sent messages. Prior work has shown that sending students low-cost text message reminders to complete pre-matriculation tasks is an effective tool for curbing summer melt.49 The text message campaign began in late June and continued through late August 2014. The research team obtained student-level demographic and academic achievement data from uAspire and student-level college enrollment information from the National Student Clearinghouse. Results. Among all students sent text messages, 68.0 percent enrolled in college in the fall compared with 64.9 percent of those not sent messages—a difference of 3.1 percentage points (p < 0.05, 95% CI [0.4, 5.8]). Enrollment effects were larger for the lowest-income students and for first-generation students. Among students with an expected family contribution of $0, for example, the text messages increased college enrollment by 5.7 percentage points (p < 0.01, 95% CI [1.4, 10.0]), from 66.4 percent to 72.1 percent. The differences between the students-only and students-and-parents treatment groups were not statistically significant.
1601-Military-Retiree-myPay-Reactivation.txt:Agency Objective. Increase military retirees’ reactivation of their online military retirement finance management platform accounts to allow retirees faster access to their tax documents and to offset operational burden for the agency during tax season. Background. One of the purposes of the online platform myPay is to help military retirees manage their finances. It allows electronic download of tax documents, such as a Retirement Account Summary (RAS) and the IRS1099 form. While myPay accounts are automatically created for all military retirees, many have never used their account, or have let their accounts deactivate by not logging in for 180 consecutive days. At the time of this pilot, about 55 percent of the retiree population had inactive myPay accounts (1.4 out of 2.5 million retirees). This poses a problem during tax season, when the number of retirees contacting the Defense Finance and Accounting Service (DFAS) surges as retirees try to either regain access to myPay or request copies of their tax documents. For example, in the fourth quarter of 2014 DFAS received a monthly average of 12,393 calls regarding accessing myPay accounts, compared to 29,046 during the first quarter (tax season) of 2015. The increased call volume during tax season places operational burden on DFAS and may result in slower services for military retirees. Methods. DFAS and OES designed nine different emails to encourage reactivation of myPay accounts prior to the beginning of tax season. A baseline email template was created using existing language about myPay from DFAS newsletters and letters to retirees, with added clear action steps on how to reactivate a myPay account. Eight additional email variants were built upon the baseline template using behavioral insights: an email with a DFAS team signature versus an email with a DFAS director’s signature, one email highlighting the resources that could be given to current service members by cutting DFAS’ mailing costs, one email appealing to the security of using myPay, one email that provided 14 social proof information, and three emails with different framings of how myPay can simplify 15 retrieval of tax documents. A control group did not receive any email initially, but received an 16,17 email after the end of the pilot. Emails were sent in four separate waves between September 8 and November 5, 2015, 18 two to three treatments at a time. Results. Between September 8 and November 10, 2015, 5.5 percent of retirees who did not receive an email reactivated their myPay accounts. In comparison, 10.2 percent of retirees who received an email had reactivated their myPay accounts—a difference of 4.7 percentage points from the control group (p < 0.01, 95% CI [4.50, 4.98]). This translates to about 800–1,000 additional customer service calls during the 3.5 months of pilot. The most effective email appealed to the security of using myPay to retrieve tax documents, compared with using traditional 19 mail. This email increased reactivation by 5.45 percentage points compared to no email over the same time period (p < 0.01, 95% CI [5.13, 5.77]). The other emails also produced significant, positive increases in reactivation relative to no 20 21 email that ranged between 2.47 and 5.00 percentage points.
1602-Public-Service-Loan-Forgiveness-Pilot.txt:Agency Objective. Increase the number of borrowers who submit Employer Certification Forms (ECFs), which keep individuals with student loans on track to receive loan forgiveness after ten years of qualifying payments. Background. The Public Service Loan Forgiveness Program (PSLF) forgives borrowers the remainder of their outstanding loan balance after making ten years of qualifying payments while working for a qualifying employer. The first borrowers will be eligible to receive forgiveness 48 starting in October 2017. To track progress towards loan forgiveness, the Office of Federal Student Aid (FSA) encourages borrowers to submit ECFs in order to verify that their employment qualifies under the program. Submitting an ECF allows for FSA to send the borrower specific information about the program. FSA collaborated with OES in 2015 on a pilot to test the effectiveness of sending emails to encourage borrowers to submit an ECF. Methods. In November 2015, FSA sent emails to a random sample of 55,221 borrowers currently enrolled in an income-driven repayment plan encouraging them to submit an ECF. FSA randomly assigned borrowers to receive an email with one of four different subject lines. An additional 54,779 borrowers were randomly selected for the control group. FSA compared email open rates across the subject lines and estimated the effects of receiving any email on 49 ECF submission rates and ECF error rates. Results. The subject lines that were most effective for getting borrowers to open emails were declarative statements. The subject lines, “Verify your eligibility for loan forgiveness” and “[Borrower name] your student loans could be forgiven” performed best with 54.0 percent of borrowers opening emails with each subject line vs. 48.3 percent for “How to get your student loans forgiven” and 46.4 percent for “The surprising way to have student loans forgiven.” Being sent any email increased ECF submissions three-fold, from 0.09 percent to 0.28 percent, a difference of 0.19 p.p., (p < 0.01, 95% CI [0.14, 0.24]). Conclusions. The pilot had encouraging results. Emails generated a three-fold increase in ECF 50 submission without increasing the error rate. FSA is using the results from the pilot to inform a large-scale PSLF email campaign targeting approximately three million borrowers that is currently in the field.
1602-Public-Service-Loan-Forgiveness-Pilot.txt:The error rate conditional on submitting an ECF for the treatment group (33.8 percent) was statistically indistinguishable from the control group (36.0 percent; difference=-2.23, p=0.78, 95% CI [-17.85, 13.39]).
1603-1-Income-Driven Repayment-Targeted-Messages.txt:E2, which prompted interested borrowers to submit an application with minimal discussion of benefits, had the highest IDR submission rate (8.9 percent) among IDR Applicant borrowers, but the difference was statistically insignificant compared to the baseline email (difference=0.31 p.p., p=0.70, 95% CI [-0.42, 1.03]). Among IDR Preference borrowers, the baseline email had the highest submission rate (9.0 percent), although there were no significant differences with the second email (difference=0.13 p.p., p=0.97, 95% CI [-0.61, 0.86]). Among borrowers who were in financial hardship, E4 had the highest submission rate. The increase was not statistically distinguishable from the baseline emails for those in forbearance, who had a 5.6 percent application rate (difference=0.36 p.p., p=0.39, 95% CI [-0.22, 0.94]), but was significantly higher than the baseline email among borrowers in deferment, increasing submissions by 0.96 percentage points from a base rate of 6.8 percent (p<0.01, 95% CI [0.29, 1.63]). Among delinquent borrowers, E5 was the only email to significantly increase IDR submission rates compared to the no-email control group’s rate of 3.45 percent (difference=0.52 p.p., p= 0.03, 95% CI [0.03, 1.01]). Among FFEL borrowers, neither E6 nor E7 significantly changed applications submission rates compared to the control group. The email presenting the benefits of REPAYE first (E6) increased applications from 6.1 percent to 6.6 percent (p=0.14, 95% CI [-.11, 1.03]), while E7 increased applications to 6.5 percent (p=0.25, 95% CI [-0.18, .96]). Conclusions. Borrowers who had previously shown interest in IDR were more likely to submit an IDR application if sent an email prompting application. Borrowers who had taken action to claim a forbearance or deferment of their loan payment due to economic hardship were more likely to submit an IDR application after receiving a targeted email using loss aversion along with a specific example of how they can pay $0 per month. Borrowers who were in delinquency and had not taken any action were more likely to submit an IDR application after receiving a targeted email containing a prompted choice. FFEL borrowers were not likely to respond to an email campaign encouraging them to proactively choose a new repayment plan.
1603-2-Income-Driven Repayment-Scale-Up.txt:Agency Objective. Increase enrollment in Revised Pay As You Earn (REPAYE) and IncomeDriven Repayment (IDR) plans among those who would benefit. application or by indicating a preference for IDR during their exit loan counseling were sent a short email starting with a prompted action and one step to leverage primacy and simplicity of action. Borrowers who were in forbearance or economic deferment were sent an email containing loss aversion language (“Avoid making monthly student loan payments of more than 10% of your income”) and using an example of the income that would result in a monthly payment of $0. Borrowers who were 31-227 days delinquent were sent an email with a prompted choice to sign up for IDR or do nothing and potentially suffer the negative consequences of default. Borrowers who had Federal Family Education Loans (FFEL), and needed to consolidate loans before being eligible for REPAYE, were sent an email that presented the benefits of REPAYE first and the eight action steps to consolidate and sign up for IDR second. Results. The short email prompting action increased IDR submissions among borrowers who had previously submitted an IDR application by 0.28 percentage point (p = 0.02, 95% CI [0.04, 0.52]) compared to a 5.93 percent application rate in the control group. The same email increased IDR submissions among borrowers who indicated a preference for IDR during exit loan counseling by 0.63 percentage point (p < 0.01, 95% CI [0.37, 0.88]) compared to a 6.81 46 percent application rate in the control group. The email containing loss aversion language increased IDR submissions among borrowers in forbearance by 0.72 percentage point (p < 0.01, 95% CI [0.53, 0.90]) compared to a 3.55 percent application rate in the control group. The same email increased IDR submissions among
1603-2-Income-Driven Repayment-Scale-Up.txt:borrowers in deferment by 1.37 percentage points (p < 0.01, 95% CI [1.01, 1.74]) compared to a 4.82 percent application rate in the control group. The email containing a prompted choice increased IDR submissions among borrowers in delinquency by 0.06 percentage point (p = 0.48, 95% CI [-0.10, .21]) compared to a 2.96 percent application rate in the control group. The email containing an explanation of benefits and action steps decreased IDR submissions among FFEL borrowers by 0.19 percentage point (p = 0.07, 95% CI [-0.01, 0.40]) compared to a 5.18 percent application rate in the control group. Conclusions. On average, sending an email increased submissions by 0.35 percentage point over the control submission rate of 4.74 percent (p < 0.01, 95% CI [0.26, 0.44]), controlling for borrower group. The campaign resulted in over 6,000 more borrowers signing up for IDR, shifting approximately $300 million of Federal student loans into income-driven repayment plans.
1604-Income-Driven Repayment-Recertification.txt:Agency Objective. Increase the number of student loan borrowers completing annual Income-Drive Repayment (IDR) recertification. Results. In Cohort 1, the message including the borrower’s actual monthly payment increase resulted in a higher recertification rate (33.9 percent) than the average monthly payment increase (difference=2.64 p.p., p < 0.01, 95% CI [2.15, 3.13]). In Cohort 2, recertification rates were indistinguishable between the group sent evenly spaced reminders (64.1 percent) and those sent reminders on consecutive days (64.3 percent; difference=0.19 p.p., p=0.53, 95% CI [0.40, 0.77]). In Cohort 3, including Cindy Battle’s signature had no effect on recertification rates. Recertification dates for those sent emails with the recertification date recertified at a marginally higher rate (64.6 percent) than those not sent the date in the email (63.8 percent; difference=0.84 p.p., p=0.06, 95% CI [-0.03, 1.71]).
1604-Income-Driven Repayment-Recertification.txt:Background. Federal Student Aid (FSA) administers loans and repayment plans for higher education. Student borrowers who enroll in IDR plans are required to complete an annual recertification process to update their income and family size. More than half of borrowers fail 47 to recertify their IDR plan each year. Those who fail to recertify are placed into the 10 year standard repayment plan which typically requires higher monthly payments than what borrowers were paying under the IDR plan. Methods. Between June and October 2015, FSA conducted a series of randomized control trials, sending emails to three separate cohorts of borrowers nearing their IDR recertification dates who would see an increased monthly payment if they did not recertify their income. Borrowers in Cohort 1 (n=142,505) were randomly assigned to be sent either an email with their actual payment increase or an email stating the average payment increase for failing to recertify. Borrowers in Cohort 2 (n=104,110), were randomly assigned to be sent an initial email and three additional emails either spaced 31 days apart or delivered on consecutive days (the day before, day of, and day after the hard deadline for filing). Borrowers in Cohort 3 (n=46,542) were randomly assigned to be sent emails either with the signature of Program Manager of Direct Loan Servicing, Cindy Battle, or no signature. Within each signature group in Cohort 3, FSA randomly included or excluded the borrower’s re-certification date in the body of the email.
1604-Income-Driven Repayment-Recertification.txt:Notes: Percentage of individuals recertifying their IDR plan in June 2015. Error bars display 95 percent confidence intervals. n = 142,505.
1605-Defaulted-Student-Loan-Borrowers.txt:Agency Objective. Help student loan borrowers in default begin a loan rehabilitation plan. Background. Each month, roughly 125,000 Federal student loan borrowers who have not made a payment in 360 days enter into default. These loans are transferred from the original loan servicer to the U.S. Department of Education’s (ED) Default Resolution Group. If a borrowers fail to act in the next 60 days, their loans are transferred to a private collections agency, and they face serious penalties including a collections fee equal to 25 percent of the principal and interest on their loan(s), damage to their credit, wage garnishment, ineligibility for future federal student aid, and forfeiture of IRS tax refunds. To avoid these penalties, ED offers a loan rehabilitation agreement that allows them to exit default if they make 9 out of 10 payments based on their income (the payment can be as low as $5). Methods. From April through July 2015, ED conducted an iterative randomized control trial, sending emails to three separate cohorts of borrowers in default, prompting them to contact the ED call center to enter a rehabilitation agreement. In Cohort 1, ED randomly assigned approximately 24,000 borrowers to be sent an email with one of 4 subject lines. The subject line with the highest open rate was used to randomly send an additional 44,000 an email that either emphasized positive collaboration with ED or the negative consequences of default. An additional 22,000 borrowers were selected for a control group. In Cohort 2, roughly 76,000 borrowers were randomly assigned to be sent either the “winning” email from Cohort 1 or one of three emails that outlined the steps for getting out of default. The emails varied the number of steps to be taken and emphasized either positive collaboration or negative consequences. In Cohort 3, ED randomly assigned roughly 80,000 borrowers to be sent either the “winning” email from Cohort 2 or an email that contained a suggested “appointment” slot for a specific lowvolume time at the ED call center. Results. In Cohort 1, the subject line simply stating “Your Student Loan is in Default” resulted in the highest open rate (19.4 percent) and the email negatively-framed email generated a higher call-in rate within three weeks (4.86 percent) than the positively-framed email (difference=1.41 p.p., p < 0.01, 95% CI [1.04%, 1.79%]) and the control group (difference=2.86 p.p., p < 0.01, 95% CI [2.52% , 3.20%]). In Cohort 2, call rates for the emails emphasizing negative consequences were statistically indistinguishable from one another, but all such emails were more effective than the email emphasizing collaboration. In Cohort 3 the subject line “We’ve Scheduled Your Appointment” resulted in a highest open rate (24.9 percent) than the previous subject line, and including the appointment led to a higher call-in rate within three weeks (7.28 percent) than the email without the appointment (difference=2.75 p.p., p Defaulted Borrower Call-In Rates in July 2015
1605-Defaulted-Student-Loan-Borrowers.txt:Notes: Percentage of defaulted borrowers calling in regarding their loan in July 2015. Error bars display 95 percent confidence intervals. n = 65,403.
1605-Defaulted-Student-Loan-Borrowers.txt:< 0.01, 95% CI [2.27%, 3.23%]). Conclusions. Prompting delinquent borrowers with a specific moment of action in the form of an appointment, increases the rate at which they
1607-Increasing-Patient-Use-of-Health-IT.txt:Agency Objective. Increase patients’ access to online patient portals and health information technology. Office operations made random assignment by patients or providers infeasible, so the new AVS was tested using a difference-in-differences estimation approach. The difference in activation rate for system primary care patients before and after August 25, 2015 was compared to the difference in activation rate of two separate patient groups during the same time periods: 1) non-primary care providers of the local health care network, and 2) primary care providers of a different health care system that uses the same online patient portal system as our partner health care system. Results. A difference-in-differences estimate suggests that the revised AVS instructions lead to a 9.98 percentage increase in the probability of online patient portal account activation 58 (p=0.07, 95% CI = [-0.69, 20.66]). Conclusions. There are promising signs that clear actions steps may encourage patients to access the online patient portal component of their health care system’s EHR. However the study can be improved with better ability to identify the study population and having more comparable treatment and comparison sites.
1608-Accountable-Care-Organization-Voluntary-Alignment.txt:Agency Objective. Encourage Medicare beneficiaries to identify and align with their main health care providers in Accountable Care Organizations (ACOs). Results. The overall response rate to the letter was 37.2 percent. The overall confirmation rate among those responding was 94 percent. There were not substantial differences in response rates across the letter designs; response rates ranged from 35.6 to 38.3 percent. The only pairwise difference that was statistically significant was between the letters with the lowest response rate (short letter without the Medicare logo) and the highest (long letter without the Medicare logo) (p < 0.05, 95% CI [0.019, 0.035]). Confirmation rates were somewhat higher for beneficiaries sent the longer letter—35 percent, compared with 31 and 33 percent for the shorter letter (without and with the logo, respectively). Conclusions. The project indicated the operational feasibility and potential for voluntary alignment of ACO patient populations. Based on this pilot, CMMI has continued to refine this approach to alignment, and voluntary alignment as a supplement to claims-based alignment is included as part of the Next Generation ACO model and the Medicare Shared Savings Program 61 Track 3. The variations in letter and form designs did not lead to significantly different responses from Medicare beneficiaries.
1609-Federal-Health-Insurance-Marketplace-Enrollment-Emails.txt:Agency Objective. Assist uninsured Americans with completing their health insurance application by sending behaviorally designed emails prior to the deadline for open enrollment. plan, individuals were randomly assigned to receive an email with one of two progress tracker graphics to mark the steps involved and the individual’s progress towards picking an insurance plan (n = 95,311). All recipients are placed at the “Pick a Plan” step, but were randomly assigned for that step to be placed either in the middle or at the last step of the tracker graphic, varying the recipient’s perception of their progress towards picking a plan. In two trials focused on individual and social motivations, content which cued personal responsibility was compared to content which cued responsibility to family. In one iteration, individuals were randomly assigned to receive either an email with personal motivations (“Do it for yourself: Submit your application”) or a generic control (“Your application is ready for review”) message in the body of the email. In the second iteration, individuals were randomly assigned to receive an email emphasizing either personal or family benefits in the email subject and body (“Benefits for you” and “Protect yourself,” compared to “Benefits for your family” and “Protect yourself and your family”). Results. In the deadline framing trial, 7.74 percent of individuals who received the 3 days framing opened the email, and 0.59 percent clicked through. For individuals who received the 72 hours framing, 8.00 percent opened the email (difference = 0.26 p.p., p < 0.01, 95% CI [0.22, 0.30]) and 0.66 percent clicked through (difference = 0.06 p.p., p < 0.01, 95% CI [0.05, 0.07]). By the February 15 deadline, 0.03 percent of individuals in each group had enrolled in insurance, with no statistically significant difference from the two framings (difference < 0.01 p.p., p = 0.95, 95% CI [0.00, 0.00]).
1609-Federal-Health-Insurance-Marketplace-Enrollment-Emails.txt:Background. During the Open Enrollment Period, qualifying individuals and families can purchase health insurance plans through the 52 Federal Health Insurance Marketplace (FHIM). For the 2015 enrollment season, the close of open enrollment was February 15, 2015. As of early February 2015, millions of people had visited HealthCare.gov and started an online account, but had not yet submitted an application and selected a plan. The Department of Health and Human Services (HHS), in collaboration with SBST, developed, sent, and tested variations of emails to assist these individuals with completing their insurance application in time. Methods. Many trials were run; we report here on a small number of trials that we selected before results were inspected. In a trial using deadline framing, three days before the open enrollment deadline, individuals who had registered for a HealthCare.gov user account but not yet enrolled in an insurance plan were randomly assigned to be sent one of two email variants encouraging them to enroll (n = 53 7,318,780). The email variants framed the time left until the deadline either as “3 days” (emphasizing numerical nearness) or “72 hours” (emphasizing deadline urgency) in the email subject and body. All other content in the email about how to enroll was held constant across both email variants. In a trial using different framing of progress toward the goal of enrolling in an insurance
1609-Federal-Health-Insurance-Marketplace-Enrollment-Emails.txt:In the goal progress trial, there were no significant effects on open or click rates of varying individuals’ perception of progress towards picking a plan. Consistent with the findings on deadline framing, we do not observe meaningful differences in enrollment: enrollment was 0.16 percent in the “middle step” group, compared to 0.14 percent in the “last step” group (difference < 0.01 p.p., p=0.51, 95% CI [0.03, 0.07]). In the first personal motivations trial, individuals who received the personal motivation email had lower open and click rates than individuals who received the generic control email. Open rates were 13.84 and 14.67 percent, respectively (difference = 0.83 p.p., p < 0.01, 95% CI [0.72, 0.94]). Click-through rates were 1.61 and 2.14 percent, respectively (difference = 0.53 p.p., p < 0.01, 95% CI [0.48, 0.57,]). Differences in enrollment rates could not be precisely estimated, with 0.03 percent enrollment in each group (difference < 0.01 p.p., p = 0.17, 95% CI [0.00, 0.01]). In the second personal motivations trial, individuals who received the personal motivation
1609-Federal-Health-Insurance-Marketplace-Enrollment-Emails.txt:Test / Group Deadline Framing 3 days 72 hours Difference p-value 95% CI Goal Progress Middle step Last step Difference p-value 95% CI Personal Motivations Personal motivation Generic control Difference p-value 95% CI Personal motivation Family motivation Difference p-value 95% CI 7.74 8.00 -0.26*** p=0.0000 [-0.30,-0.22] 15.42 15.79 -0.38 p=0.1095 [-0.84,0.08] 13.84 14.67 -0.83*** p=0.0000 [-0.94,-0.72] 12.36 11.73 0.63*** p=0.0000 [0.50,0.77] 0.59 0.66 -0.06*** p=0.0000 [-0.07,-0.05] 1.75 1.68 0.01 p=0.4235 -0.10,0.23 1.61 2.14 -0.53*** p=0.0000 [-0.57,-0.48] 0.60 0.59 0.01 p=0.6987 [-0.02,0.04] 0.03 0.03 0.00 p=0.9501 [-0.00,-0.00] 0.16 0.14 0.00 p=0.5106 [-0.03,0.07] 0.03 0.03 0.00 p=0.1706 [-0.01,0.00] 0.01 0.00 0.01*** p=0.0051 [0.00,0.01] 47,711 47,600 3,659,230 3,659,550 Open (p.p.) Click (p.p.)
1609-Federal-Health-Insurance-Marketplace-Enrollment-Emails.txt:email had higher open rates than individuals who received the family motivation email: 12.36 percent compared to 11.73 percent (difference = 0.63 p.p., p < 0.01, 95% CI [0.50, 0.77]). There was no detectable effect on click rates between the groups (difference = 0.01 p.p., p = 0.70, 95% CI [-0.02, 0.04]). Enrollment rates were 0.014 percent and 0.008 percent in the personal and family motivation groups, respectively (difference = 0.006 p.p., p < 0.01, 95% CI [0.00, 0.01]), a statistically significant difference though small in magnitude. Conclusions. Variations in framing an email about health insurance enrollment—hours until deadline rather than days; generic email rather than personal motivation; personal motivation rather than family motivation—can make individuals more likely to open their email and click through. However, even precisely estimated differences in open and click rates do not translate into large differences in health insurance enrollment rates, which may reflect some of the limitations of email campaigns. That is, email may have a limited impact on behaviors like enrollment.
1609-Federal-Health-Insurance-Marketplace-Enrollment-Emails.txt:Notes: For each of the three sets of trials described above, the table shows levels and differences across treatment conditions for three outcomes: email open rates, click rates, and enrollment rates. * p<0.10; ** p<0.05; *** p<0.01
1610-Tax-Filing-and-EITC-Take-up.txt:individuals who filed but did not claim the EITC. A sample of 360,352 was drawn from the population of roughly individuals estimated to be eligible but who failed to file in either 2011 or 2012; of this group, 199,910 were randomly assigned to a treatment and 160,442 to control. Results. The notices lead to an increase in the rates of individuals filing a 2013 tax return between 0.6 and 1.0 percentage point, depending on the model. A simple comparison of filing rates between the treatment and control groups suggests a 1.0 percentage point increase in filing (p < 0.01, 95% CI [0.006, 0.014]), while a model that incorporates other factors associated with filing such as gross income and potential EITC suggests a treatment effect of 0.6 percentage points (p < 0.01, 95% CI [0.002, 0.01]). The impact was largest for those sent a postcard early in the tax season. Conditional on filing, there were not significant differences in the fraction of individuals claiming the EITC. However, treatment individuals did receive a greater EITC in 2014, by about $25 on average (p < 0.01, 95% CI [13.2, 36.4]).4 Conclusions. The notices were effective at modestly increasing the rate of tax filing among the targeted population; while they did not increase the rates at which individuals claimed the EITC, they did increase the amount of EITC dollars paid to treatment individuals. Interestingly, the effect of the notices appeared to be transitory. The study tracked the effects on filing a year later, in 2015, finding no differences in filing rates between treatment and control individuals. The project also sent reminder notices in the second year to a randomly selected subsample of individuals who filed in year one of the study; the reminder notices increased the rate of tax filing, suggesting the importance of ongoing communications and reminders.
1611-Climate-Indicators.txt:Agency Objective. Assess and improve the understandability of graphical indicators of climate change by reducing complexity and displaying descriptions of key indicator messages. Background. The U.S. Global Change Research Program (USGCRP) makes available figures of climate data, including a set of indicators. Developed by climate impact experts and stakeholders, the indicators are intended to communicate scientific facts, inform decision making, and illustrate progress and change. The effectiveness of indicators for informing the public and decision makers depends in part on their understandability: their abstractness, complexity of patterns portrayed, and graphical techniques. The research team gauged the understandability of 14 existing USGCRP indicators using an online survey of a sample of the U.S. population. For each indicator, approximately 100 respondents were asked between three and six questions designed to gauge (1) how successfully indicator information was interpreted, and (2) whether this information was used correctly in making inferences about their meaning. The two indicators that had the lowest understandability were selected for redesign. Methods. Two versions of the Annual Greenhouse Gas Index indicator were created: the first eliminated one of the y-axes, and the second changed the title to reflect the key message of the indicator. The Annual Heating and Cooling Degree Days indicator was redesigned by depicting two related trends as separate graphs instead of a single paired bar graph. A second online survey asked new samples of 75–100 respondents the same questions to test whether the design changes had an effect on understandability. Results. For the Annual Greenhouse Gas Index, eliminating one of the y-axes increased successful interpretation of the indicator by 18 percentage points, from 57 to 75 percent (p < 0.01, 95% CI [5.43, 29.96]) but did not improve correct inferences (p = 0.42, 95% CI [-7.56, 18.26]). Using a more descriptive title did not lead to a significant improvement in interpretation or inference. For Annual Heating and Cooling Degree Days, separating paired bar graphs did not improve interpretation but did increase correct inferences by 19 percentage points, from 47 to 66 percent (p < 0.01, 95% CI [5.04, 33.26]). Conclusions. Indicator effectiveness is best achieved through clarity of the visual key message rather than increased complexity in a
1612-2-nslp-rolling-verification.txt:requests are compared to the rates for  business-as-usual verification requests in Broward  County, Los Angeles USD, San Diego USD and  Prince George’s County, controlling for the date  that applications were submitted and the initial  eligibility determination for each applicant (either  free or reduced-price meals).   Overall response rates increased by an average of  2.5 percentage points (p=0.11, 95% CI [-0.01, 0.06]) 
1612-2-nslp-rolling-verification.txt:for applicants selected using rolling verification.  When analyzing each district separately, response  rates increased for applicants receiving verification  soon after applying in three districts (Broward, San  Diego, and Prince George’s) but decreased in Los  Angeles.   Rolling verification response rates were 7.4  percentage points higher in Broward County  (p=0.03 , 95% CI [0.01, 0.14]),  2.7 percentage  points higher in San Diego USD (p=0.72 , 95% CI  [-0.12, 0.17]), and 4.4 percentage points higher in  Prince George’s County (p=0.27, 95% CI [-0.03,  0.12]). Response rates in Los Angeles USD were 3.9  percentage points lower among rolling verification  recipients (p=0.06, 95% CI [-0.08, 0.0]). Effects of  rolling verification could not be evaluated in  Orange County because all applicants received  requests shortly after submitting their applications.    
1612-National-School-Lunch-Program-Verification.txt:Agency Objective. Increase household response to requests by Local Education Agencies (LEAs) to provide materials supporting applications for benefits delivered under the National School Lunch Program (NSLP). Background. NSLP is a federally assisted meal program operating in over 100,000 public and non-profit private schools and residential child care institutions. It provided nutritionally balanced, low-cost or free lunches to more than 22 30 million children each school day. USDA’s Food and Nutrition Service (FNS) administers the NSLP at the Federal level, while state and local authorities deliver the program to children in schools. As part of the NSLP program, LEAs are required to confirm the eligibility of some household recipients for free and reduced-price 23 meals. Prior studies have demonstrated that some of the households selected for verification do not respond to requests to provide proof of eligibility not because they lack these materials, but because of misunderstanding or confusing 24 procedures. Methods. In order to make the request for verification clearer and easier to understand, the research team designed a new request letter that used simpler language, pictures meant to cue an association with the NSLP, a new visual summary of materials that the household was required to submit, and a personalized message from the LEA. In addition, the letter emphasized that materials could be submitted in multiple formats, including by smart phone (households could take pictures of required documents and email them to the LEA using their smart phone). Finally, rather than conduct the verification procedure in October—two months or more from the beginning of school when benefits begin to be delivered for most families—FNS considered conducting verification on a rolling basis, requesting verification materials from households as the household’s application is approved for benefits. Ultimately it was impossible to implement each of the planned changes at once, since the verification procedure is independently carried about by each of the nearly 20,000 LEAs across the country. Revised letters were used in 74 LEAs across the country, balanced by 82 LEAs that were randomly paired with the pilot LEAs but were not asked to revise their letters. Some, but not all, of treatment LEAs were able to implement smartphone submission. None of the treatment LEAs were able to implement changes in the timing of their verification procedures during year one of the 25 pilot. We report here on what we term a “feasibility pilot” meant both to generate preliminary data on the impact of revised letters, as well as to learn about procedural variation across LEAs. Results. Using a difference-in-difference estimator, we estimate that the new letters increased response by 2.1 percent (p = 0.66, 95% CI [-7.10, 11.28]), though this is imprecisely estimated. Conclusions. Initial work in this area suggests that improving the design and clarity of communications to beneficiaries may be helpful in increasing response to verification requests, but also suggest that more substantial changes to the verification procedure will be necessary to
1613-Local-USDA-Elections.txt:Agency Objective. Increase voter participation in non-partisan County Committee Elections. Background. The Department of Agriculture’s (USDA’s) Farm Service Agency (FSA) operates programs—such as loans, disaster payments, and commodity and conservation programs—that impact the lives of farmers and ranchers, their income, and the economy. FSA interacts directly with farmers and ranchers through a network of local field offices, where farmers can inquire about or apply for programs. In addition to being a point of contact between FSA and farmers, important policy decisions are made at the local level, including setting payment rates. Each field office is administered by a County Executive Director who is responsible for the local implementation of FSA programs. The County Executive Director is in turn overseen by a County Committee (COC) whose members are elected by all farmers eligible to participate in FSA programs. County Committees were first authorized by the Soil Conservation and Domestic Allotment Act of 1935. Over time, participation in COC elections has declined, endangering the model of local representation that the Committees represent. In an effort to increase voter turnout, FSA partnered with ERS and the Office of Evaluation Sciences (OES) to test changes to COC election ballots and outreach material. Methods. The experiment was conducted during the 2015 COC elections, which took place by mail over an approximately one-month period in late 2015. FSA mailed a ballot to each eligible voter in early November; the deadline for voters to return a valid ballot was approximately one month later, in early December. Two changes to voter outreach were tested in the experiment: (i) candidate information printed on the outside of ballots and (ii) postcards with candidate information sent to voters (n = 1,399,307). First, because voters receive ballots by mail, one barrier to submitting a valid ballot may simply be the action of opening the ballot and evaluating candidate choices. We printed the names of candidates—which are otherwise included only on the inside of sealed ballots—on the outside of some ballots so that they would be readily apparent to eligible voters regardless of whether or not they opened the ballot. Second, because voters may simply forget to vote by the deadline, even if they intend to, we tested the effect of informational postcards bearing the candidates’ names and information about the election on voter turnout. A total of two postcards were sent to all eligible voters who were assigned to the relevant treatment condition, one designed to arrive approximately one week before the ballot arrived in the mail, and one designed to arrive approximately one week before the ballot submission deadline. The pre-ballot postcard included: (i) the names of all candidates running for election; (ii) a personalized message encouraging eligible voters to help make sure farmers in their county were represented; (iii) a reminder that the term of the Committee Member would be three years in length, implying that the next chance to vote for COC representation would be three years in the future; and (iv) a picture of the ballot that would be arriving in the mail soon. The picture provided a visual cue that the eligible voter would associate with the postcard and the election when they received the ballot in the mail. The post-ballot postcard included all the same information that the pre-ballot postcard did, and additionally provided text reminding the eligible voter that the deadline was approaching. Voters were also informed that if the ballot had been lost (or failed to arrive in the mail), the eligible voter could obtain a new ballot by visiting their local field office. Results. The voter participation rate of households that received neither an enhanced ballot nor a postcard was 9.3 percent. The
1613-Local-USDA-Elections.txt:treatment effect of including information on the ballot and sending postcards is estimated to be 2.9 percent (p < 0.01, 95% CI [2.7, 3.0]), or a relative effect of nearly 24 percent. To put the estimated treatment effect into perspective, with a treatment effect of 2.9 percent and a postcard cost of approximately $0.05 per unit, this translates to one extra ballot cast for every $1.72 spent. FSA can use this information to encourage participation in future elections, and can build on the results here to create new lowcost outreach strategies. Conclusions. Providing information to farmers, as well as reminders, can increase participation in the democratic process to elect local representation.
1614-SBA-Learning-Center.txt:Agency Objective. Increase enrollment in the Small Business Administration (SBA) Learning Center by reducing frictions associated with enrollment forms. military may qualify for particular services, for 28 example. Methods. SBA and OES created two updated versions of the registration form that both decreased the free-form fields to just one, decreased the number of multiple choice questions to six (while still collecting the same information), and reordered the form to begin with simple questions about the small business rather than name, address, and ethnicity of the individual filling out the form. SBA piloted the forms during three months beginning March 25, 2016 and recorded the proportion of users who continued to the course and the proportion of users who left the website without completing the form. Results. During the three months that the new forms were piloted, 64.1 percent of users continued the course after registering, compared with a 57.6 percent in the three months before the changes were implemented, an increase of 6.5 percentage points (p < 0.01, 95% CI [5.81, 29 6.81]). The second updated version of the form also appeared to increase the amount of information voluntarily provided during registration. In the two months before the change, 77.8 percent of visitors provided some or all of the information requested, compared to 87.4 percent of those presented with the new form, a 9.6 percentage
1614-SBA-Learning-Center.txt:point increase (p < 0.01, 95% CI [9.14, 10.18]).
1614-SBA-Learning-Center.txt:Compared to a similar two-month period in 2015, the percent continuing onto the course was 6.2 percentage points higher (p < 0.01, 95% CI [5.73, 6.91]). The figure titled “Information Provision” displays all available data, and includes a small gap in May–June 2015 when data are unavailable. 31 Data was not available for May 24, 2015–July 24, 2015.
1615-Financial-Aid-for-HUD-Assisted-Youth.txt:from the behavioral literature. The personal story from First Lady Michelle Obama was intended to motivate educational aspiration and reduce the anxiety low-income students may 36 feel when thinking about college, and the postcard presented information without 37 requiring the recipient to open an envelope. Approximately 5,000 individuals were sent each of the nine variations; the control group includes approximately 160,000 individuals (n = 203,191). Results. All youth and full time students 17 to 20 as of January 2016 were included in the trial, and random assignment was performed without knowing which individuals already submitted a FAFSA for the 2016–2017 academic year. The mailings were sent on March 24, 2016 so we only consider individuals who had not submitted a FAFSA by the mailing date in the results 38 below. There are no significant effects of sending mailings on the rate of FAFSA completions. Twenty-two percent of individuals who were sent some kind of mailing completed the FAFSA, which was 0.30 percentage point higher than the rate of the control group (p=0.24, 95% CI [-0.20, 0.80]). The most effective sender was the First Lady using her personal story, with a 22.4 percent FAFSA completion rate, 0.65 percentage point higher than the control group (p=0.11, 95% CI [-0.16,
1615-Financial-Aid-for-HUD-Assisted-Youth.txt:1.46]). The letter including the paper FAFSA was the most effective format, also with a 22.4 percent completion rate (0.69, p=0.10, 95% CI [0.12, 1.50]). Conclusions. The effects of the mailings were not large enough to say with confidence that they increased FAFSA completion rates but there is suggestive evidence that some messengers and formats were more effective than others. The most effective messenger appeared to be the First Lady using her personal story, but the least effective was the First Lady using FSA’s language. This could suggest that the messenger was more effective when the message seems authentic. The most effective format was including the paper FAFSA. From a behavioral perspective, this format likely was effective because it made it easier for recipients to follow through on intentions that they already held.
1615-Financial-Aid-for-HUD-Assisted-Youth.txt:95% CI [-0.20, 0.80] [-0.30, 1.30] [-1.04, 0.56] [-0.15, 1.45] [-0.46, 1.14] [-0.11, 1.49] [-0.91, 0.68]
1615-Financial-Aid-for-HUD-Assisted-Youth.txt:Notes: Each row reports the estimated difference between the FAFSA completion rate for the indicated group and that of the control group in percentage points, the associated p=value and 95% confidence interval. The control group FAFSA completion rate was 21.7 percent.
1616-Process-Improvements-for-Virtual-PEER-Forums.txt:Agency Objective. Increase the awareness and utilization of Virtual PEER Forums and online resources for military caregivers using an email that directs mailing-list subscribers to an interactive web-based activity. Background. Virtual PEER (Personalized Experiences, Education, and Resources) Forums provide an opportunity for those caring for ill and wounded service members to meet remotely with their peers in real time to provide and receive input in the form of knowledge and resource sharing, as well as socioemotional support. The Offices of Military Community and Family Policy and Warrior Care Policy sought to increase awareness and utilization of Virtual PEER Forums by those who are eligible to participate, along with increasing general awareness of the online resources available to this population. Methods. The Department of Defense (DoD) Office of Warrior Care Policy (WCP) worked with OES to explore improvements to existing communication about monthly Virtual PEER Forums for military caregivers. The team designed an activity for engaging blog subscribers in the PEER Forums. In collaboration with WCP, OES designed two primary changes to the existing outreach strategy. First, the team used language (in both the subject line and email body) that emphasized caregivers receiving support from their peers versus giving support to their peers. All individuals who self-subscribed to an email mailing list maintained by WCP received one of two versions of the email, emphasizing either receiving or giving support. In addition, OES designed an interactive webbased activity to encourage individuals to more actively engage after clicking on a link in the email. This link directed individuals to a short (2 minute) web activity where they were able to self-assess their knowledge of the resources 55 provided by WCP. Results. The test was conducted in July 2016. Among the group receiving emails emphasizing the receipt of support, 5,927 emails were delivered, leading to a 16.4 percent unique open rate and a 2.4 percent click-through rate to the web-based activity. Among the group receiving emails emphasizing giving support, 5,926 emails were delivered with a 15.3 percent unique open rate and a 1.9 percent click-through rate, a difference of 1.1 percent (p=0.1, 95% CI = [-0.20, 2.43]), and 0.5 percent (p=0.05, 95% CI = [0.00, 56 1.04]) respectively. Forty-seven percent of individuals who began the web-based activity completed it, and five new caregivers signed up for the July forum— three from the group who received emails emphasizing giving support and two from the group who received emails emphasizing receiving support. In addition, during the three-day period directly after the emails were sent, WCP reports that the daily views to the Caregiver Resource webpage increased by 70 percent relative to the average daily views in 2016 to date. Conclusions. The test comparing receiving and giving language suggested that highlighting the benefits to subscribers increased email open rates, though not click-through rates.
1617-Military-OneSource-Subscription.txt:importance reminders. of ongoing communications and Results. Overall, the study added over 6,000 new subscribers to the newsletter, expanding the subscription list by more than 10 percentage points; the study spurred 8,700 web visits across web browsers and devices. Average rates of subscription across treatment conditions were generally low (between 1.19 and 1.58 percent). Emails that presented the decision to subscribe as an active choice (i.e. “Yes I want to take advantage of the eNewletter benefits” vs. “No, I prefer not to stay in the loop”) outperformed those that simply provided a web link, generating an average subscription rate of 1.43 vs. 1.25 percent, a difference of 0.17 p.p. (p < 0.01, 95% CI [0.11,0.24]) and a rate of new website visits of 1.9 vs. 1.6 percent, a difference of 0.3 p.p. (p < 0.01, 95% CI [0.2,0.4]). The most effective communication strategies excluded lists or quizzes and kept the emails short (1.43 vs. 1.29 percent subscriptions (difference = 0.15 p.p., p < 0.01, 95% CI [0.08, 0.22]) or 2.1 vs. 1.6 percent new website visits, (difference = 0.5 p.p., p < 0.01, 95% CI [0.4,0.6])). Conclusions. Simple, clear communications that present decisions as an active choice are effective at increasing subscriptions. The way in which the benefits of that action are presented also matters. Lists appear more effective than quizzes, but brevity is most important.
1617-Military-OneSource-Subscription.txt:emails sent and involve no missing data. All comparisons reported here are statistically distinguishable at p<.05.
1618-On-Base-Active-Choice-for-TSP.txt:Results. During the five-week period including the pilot at TSP Enrollment Rates at Pilot and Comparison both bases, the enrollment rate Bases Before, During, and Aher the Pilot was 10.74 percent at Fort Bragg 14% and 8.39 percent at Fort Lewis, compared to a maximum of 1.86 12% Pilot period percent at the other three 10% bases. We use a linear probability model to estimate 8% that the pilot led to a 8.32 6% percentage point increase in the likelihood of a service member 4% enrolling in TSP within four weeks of in-processing (p < 2% 6,7 0.01, 95% CI [7.13, 9.51]). 0% There is also some evidence, as 12/27/2014 4/6/2015 7/15/2015 10/23/2015 1/31/2016 5/10/2016 seen in the figure below, that Bragg Lewis Hood Campbell Benning the Fort Bragg intervention—an active choice on a paper form— led to a larger effect size than the computerenrolling within four weeks of inprocessing (p < 8 based enrollment intervention at Fort Lewis. 0.01, 95% CI [8.95, 10.81]). If we restrict the data to examine service members likely to have been in the service for four years or less, and thus more likely to be making initial decisions about retirement savings, we estimate that the pilot led to a 9.88 percentage point increase in the likelihood of
1620-2-secure-messaging.txt:numbers of registered patients over the course of  the trial period, and again found no significant  difference between conditions. (Error bars in the  graph indicate 95% confidence intervals.) 
1701-microloans-women-owned-farms.txt:(pp)  (p<0.05,  95%  CI  [0.02pp, 0.1pp]) more likely to  apply  for  a  microloan  than  farmers  who  did  not  receive  the  mailer.  In  2016,  this  result  did  not  replicate.  Using  a  linear  regression  model  to  analyze  differences  in  loan  application  rates  of  female  farmers  between  paired  counties,  OES  estimated  the  direct  effect  of  the  mailer  to  be  a  difference  of  -0.013pp  (95%  CI  [-0.04pp,  0.014])  ,  which  was  not  statistically  significantly  different  from  zero.  (The  loan  application  rate  among  farmers  in  the  control  group  was  0.42%.)  In  sum,  OES  found  no  statistically  significant  effect  of  the  direct mailer on the farmers who received it.  In  addition,  OES  estimated  the  spillover  effect  on  farmers  near  those  who  received  the  mailer  to  be  -0.029pp  (p<0.05,  95%  CI  [-0.006pp, -0.002pp])  by  comparing  loan applications of farmers who lived in  treated  counties  but  did  not  receive  mailers  with  those  in  control  counties.  Though  the  effect  is  small,  this  result  provides  some  evidence  that  spillover  might  have  had  the  opposite  effect  as  expected.  Farmers  who  lived  near  recipients of the  mailer  were  slightly  less  likely  to  apply  for  microloans  than  farmers  in  counties  where  no  one  received the mailer.       
1702-Project-MGMT.txt:intervention from manager.    Results  The  Growth  Mindset  tools  did  not  have  any  detectable  effect  on  employee  or  self-assessments,  although  the  effects  are  imprecisely  measured  due  to  employee  and  manager  response  rates.  For  example,  the  change  in  average  employee  assessment  among  the  managers  in  the  Growth Mindset  group was  about  .17  smaller  than  the  corresponding  change  in  the  managers  in  the  control  group  [p=.3,  95%  CI  -.49,  .15].  This  difference  (shown  in  the  third  boxplot on  the  right  of  Figure  1),  is  mostly  driven  by  the  fact  that,  by  change,  the  baseline  scores  of  the  managers  in  the  treatment  group  were  a  bit higher  than  the  managers  randomly  assigned  to  the  control group. 
1704-USAJobs-abstract.txt:demographics information the most. In Variant 1  (control group), the average rate of submission of  demographic information was 59.7%. This rate was  9.4 percentage points higher in Variant 2 (p<0.001,  95% CI = [9.1, 9.7]), and 11.2 percentage points  higher in Variant 4 (p<0.001, 95% CI = [10.9, 11.5]).   
1706-Students-at-Risk-to-Withdraw.txt:school?” (34.0 percent, difference=6.0 ppt, p<0.01,  95% CI [4.69,7.32]).   Borrowers emailed in the second phase were  less  likely to be enrolled the following spring semester  (37.5 percent) than those who were not emailed  (38.5 percent, difference = 1.0 ppt, p<0.01, 95% CI  [-1.4, -0.5]). The results persisted through the fall  2016 semester, with emailed borrowers 0.9  percentage points less likely to be enrolled (31.3  percent compared to 30.3 percent, p<0.01, 95% CI  [-1.4, -0.5]).  
1706-Students-at-Risk-to-Withdraw.txt:Additional analyses suggest that the decrease in  enrollment was concentrated among public  (decrease of 1.33 ppt, p<0.01) and private,  not-for-profit institutions (decrease of 1.56 ppt,  p=0.05). There were no significant differences in  enrollment when looking at for-profit institutions  alone. The results also suggest an interaction with  family income, with larger decreases in enrollment  among those with lower incomes.    Conclusion Emails reminding borrowers of  financial aid and loan repayment decisions  decreased enrollment in the second and third  program years. Given that the borrowers included  in the study were at risk to withdraw, it is possible  some made the decision not to return to school  earlier than they otherwise would have.  
1709-Quetiapine-Prescribing.txt:who were sent placebo letters (p < 0.001 , 95% CI  [9.2%, 13.1%]). Baseline patients of prescribers who  were sent peer comparison letters received 3.9  percent less quetiapine than patients of prescribers  who were sent placebo letters (p < 0.001, 9  5% CI  [2.9%, 5.0%]). This reduction in quetiapine receipt  was significantly larger among patients who were  low value candidates for quetiapine than patients  with guideline concordant indications (test of  difference: p = .01): The reduction was 5.9 percent  for patients with low value indications (p < 0.001,  95% CI [3.9%, 8.0%]) and 2.4 percent for patients  with guideline concordant indications (p = 0.002,  95% CI [0.9%, 4.0%]). Analyses did not detect signs  of prescriber “gaming” of study outcomes, and there  was no evidence of patient harm based on  hospitalization and emergency department visits.    Figure 1. Quarterly Quetiapine Prescribing in  Treatment and Control Groups  comparison letters to high  prescribers of quetiapine reduced  prescribing, and did so without any  detectable adverse impacts.  Considering the cost to Medicare Part D of  antipsychotics like quetiapine and concerns about  over-prescribing of these and other drugs, low-cost  interventions like this one offer the potential to  reduce spending in Medicare while raising the  quality of health care that beneficiaries receive.  
1709-Quetiapine-Prescribing.txt:* Each point is the average days of quetiapine supplied by prescribers in the  quarter. Error bars indicate 95% confidence intervals. Arrowheads denote  when letters were sent to prescribers. 
1710-increasing-pre-post-natal-care-abstract.txt:period. The control group (N= 4,165) had a take-up  rate of 5.3 percent (95% CI [4.77% to 5.83%])  versus the treatment (N= 2,681) rate of 6.5 percent  (95% CI [5.92% to 7.08%]). These differences were  not statistically significant, and we see no  differences with an Intent-to-Treat versus  Treatment-on-the-Treated model. The primary  driver of service take up appears to be phone  contact between the referred individual and NFP  nurse.  
1710-reaching-low-income-mothers-abstract.txt:control group (N= 4,165) had a take-up rate of 5.3  percent (95% CI [4.77% to 5.83%]) versus the  treatment (N= 2,681) rate of 6.5 percent (95% CI  [5.92% to 7.08%]). These differences were not  statistically significant, and we see no differences  with an Intent-to-Treat versus  Treatment-on-the-Treated model. The primary  driver of service take up appears to be phone  contact between the referred individual and NFP  nurse.  
1713-supporting-homeless-students-abstract.txt:tested with a randomized controlled trial between  January and May 2017. LEAs (n = 1,732) were  blocked by state, if they had reported homeless  students in School Year 2015-2016, and if they  were a charter school. LEAs were randomly  allocated to receive the status quo (less frequent,  more formal, traditional emails) or more frequent  behaviorally informed emails.  
1713-supporting-homeless-students-abstract.txt:year, LEAs in the intervention group identified on  average 3.6 more homeless students (p = 0.07, 95%  CI[-0.31, 7.56]),5 a 12 percent relative change  compared to the control group baseline mean. This  represents identifying over 3,000 more students to  receive support through the EHCY program in the  study sample. However, the intervention did not  make districts more likely to identify homeless  students if they had not previously done so.  designed to capture differences between the  states, conclusions cannot be drawn about how the  unique circumstances in each state may have  contributed to the overall results. Future work  could explore why the intervention had differential  results across each of the three states.   The results suggest the intervention was effective  because it gave liaisons a clear set of well-timed  action steps, in a context where they previously  received a large amount of dense information.  More work could explore this more deeply and  confirm such results hold across contexts.  
1716-Increasing-Student-FAFSA-Renewal-Rates.txt:rate=4.34%, p < 0.01, 95% CI [3.34, 3.51]).  The  shape of Figure 2 suggests that while the effects of  an email may be largest right after it is sent – the  rate increases are largest one day after the email is  sent – they persist over time. There is a nearly  linear increase in completion rates across the full  20 day time period. The estimated effect is  restricted to 20 days. It is possible the email  induced those who would have completed the  FAFSA in the absence of the email to complete it  sooner. Even if the emails only generate a  pull-forward effect, they can still be valuable. 
1722-project-abstract-1-month.txt:tested with an individual level randomized control  trial. All 866 study participants, HIV+ individuals  not yet on ARTs, consented into the study.  Approximately one-half of participants (n=436)  were randomly assigned to receive the encouraging  phone call and approximately one-half (n=430)  were randomly assigned to the standard of care.  An ordinary least squares (OLS) regression was  used to compare retention in care and ART  adherence at 1 month.8  
1727-va-health-care-benefits.txt:significant increase in enrollment within 30 days  post-separation among service members assigned  to receive email outreach. Enrollment in VA health  benefits was 10.7% in the control group and 10.9%  in the pooled treatment arms. The difference  between the two enrollment rates is not  statistically significant (p=0.74, 95% CI [-.009,  .012]). We can rule out an increase in enrollment as  small as 1.2 percentage points due to the email  campaign.3  For combat veterans, we observe that 
1727-va-health-care-benefits.txt:enrollment within 30 days is 11.9% in the control  arm and 12.4% in the pooled treatment arm; again,  the difference is not statistically significant (p=.45,  95% CI [-.008, .017]). 
1727-va-health-care-benefits.txt:statistically significant (p=.000, 95% CI [-.031,-.009]),  suggesting lower open rates in the formal arm.  4  As previously noted, a large number of TSMs were  excluded because their email addresses were not  available. In addition, estimates from other Concierge for  Care projects suggest that up to one-third of individuals  identified as TSMs by the Department of Defense did not  separate from the military during the pilot period. 
1733-project-abstract.txt:targeted communications around FSA enrollment  did not lead to increased enrollment. Enrollment in  FSAs was 27.4% in the control group and 27.2% in  the pooled treatment arms. The difference  between the two enrollment rates is not  statistically significant (p=.838, 95% CI [-.021,  .017]).  The average amount contributed over the  2018 calendar year is $686.49 in the control group  and $683.66 in the pooled treatment arms, and  Unless noted otherwise, all of the analysis reported in this  abstract was prespecified in an analysis plan, which can be found  at https://oes.gsa.gov.
1733-project-abstract.txt:again the difference is insignificant (p=.929, 95% CI  [-65.053, 59.395]). We can rule out an increase in  the probability of enrollment as small as 1.7  percentage points due to the email treatments. In  addition, no significant differences are evident  comparing across the treatment arms. Figure 1  shows the probability of contributing to a FSA by  experimental arm, in conjunction with the 95%  confidence interval; Figure 2 similarly shows mean  contributions by experimental arm. 
1735-flu-shot-abstract.txt:targeted communication around the importance of  accessing the flu vaccine did not lead to increased  uptake of the vaccine. In the full experimental  sample, uptake of the flu vaccine was 40% in the  control group and 38% in the treatment arm. The  difference between the two enrollment rates is  statistically insignificant (p=.475, 95% CI [-.056,  .026]). In addition, due to incomplete baseline  information around prior uptake of the flu vaccine,  some patients included in the experiment had  received the vaccine prior to the launch of the  intervention. When we restrict the sample to  exclude these early adopters, uptake of the flu  vaccine during the post-intervention period was  18% in the control group and 17% in the treatment  arm, and the difference between the two  enrollment rates is statistically insignificant  (p=.923, 95% CI [-.035, .039]). Figures 1 and 2 show  the probability of flu vaccine uptake for patients in  the control and treatment arms for the full sample  and the restricted sample, respectively. 
1737-vaccine-report-cards.txt:compliance was 76.3% among schools that received  the report cards and 76.2% among school that did  not (p = 0.836, 95% CI[-2.30, 2.84]). The same null  results hold among day care, elementary, middle, and  high schools, for each vaccine type. This result is  consistent with the hypothesis that report cards in  the absence of incentives for school leaders are a  relatively weak mechanism for stimulating change, a  conclusion that is consistent with existing evidence  around interventions targeting principals. 
1739-gsa-auctions.txt:Results The figure shows that the treatment  group maximum bid amount was about  $7,670,000 more in total than the control  group during this period (in which the  experiment sent approximately 800,000  emails to approximately 37,000 users across  7120 lots).2 Within lot, the treatment group  paid more, on average than the control group  98% of the time. OES estimated the effect of  the emails on participation in the targeted  auction (i.e. bidding more than zero) to be .08  percentage points (approximately .05 % of the  control group bid and about .13 % of the  treatment group bid) ( 95% CI [.06,.09], p <  .001). The effect of the program on maximum  bid amount was about $20 on average but this  difference was not precisely estimated given  the huge number of zeros and the few  extremely large bids: the maximum bids  ranged from $0 to $2.6 million, 50% of the  non-zero bids were between about $10 and  $500 with only 10% more than $11,000) (95%  CI [-17,67], p=.3). Although this research  design had little information available for  statistical tests of the average or even total  differences,  we can deﬁnitely reject the null  hypothesis that the treatment group bid the  same amount as the control group if we  compare the rank of the treatment group bids 
1740-flu-shots-va-st-cloud.txt:differences among the three  postcards in either uptake or timing  of flu shots. In each of the three  groups, flu shot uptake was about  40%. After a pre-specified  adjustment for individual characteristics including  age, rurality, and prior flu shots, the difference  between each behaviorally informed postcard and  the basic postcard was not statistically significant  (motivation postcard: p = .27, 95% CI = [–.014,  .004]; implementation postcard: p = .53, 95% CI =  [–.012, .006]). The results rule out a difference as  small as 0.9 percentage points between each  behaviorally informed postcard and the basic  postcard. The three groups were also similar in  timing. In all three groups, those who got flu shots  got them on average 38 days after postcards were  sent (median = 27 days). The difference in average  days elapsed from mailing to flu shot between each 
1740-flu-shots-va-st-cloud.txt:behaviorally informed postcard and the basic  postcard was not statistically significant  (motivation postcard: p = .42, 95% CI = [–0.73,  1.75]; implementation postcard: p = .87, 95% CI =  [–1.33, 1.13]). 
1742-reducing-energy-costs.txt:expected receipt of the flyers. The sample includes  2,514 units with 11 or more days of energy use  data during the outcome period.5 Average daily  energy use recorded by WEMs during the outcome  period was compared for units sent no flyer to units  sent a flyer with a single or multiple energy-saving  tips. All comparisons controlled for mean baseline  energy use during a two-week period in late August  and early September 2017 and assignment block  fixed effects (42 development--bedroom  combinations).      that follow are reported both  without adjusting for multiple  comparisons and adjusting for  multiple comparisons using the  Holm-Bonferroni family-wise error rate. Units sent  flyers with either a single tip or multiple tips used  less energy per day during the two weeks following  expected receipt of the flyers compared to units  not sent flyers, although the difference was not  statistically significant. On average, energy use for  the control group was 16.88 kWh per day; those  sent the single-tip flyer used 16.56 kWh per day,  and those sent the multiple-tip flyer used 16.72  kWh per day. Energy use among units sent any flyer  (single- or multiple-tips) was 0.235 kWh per day  lower than units in the control group, but this  difference was not statistically significant (p=0.14  (Holm-Bonferroni-corrected p=0.42) , 95% CI  [-0.55, 0.08]). Differences between the single-tip  flyer and control (-0.317 kWh per day, p=0.08  (Holm-Bonferroni-corrected p=0.35), 95% CI  [-0.68, 0.05]), multiple-tip flyer and control (-0.155  kWh per day, p=0.40 (Holm-Bonferroni-corrected  p=0.77), 95% CI [-0.52, 0.21]), and between  single-tip flyer and multiple-tip flyer (-.162 kWh 
1742-reducing-energy-costs.txt:per day, p=0.39 (Holm-Bonferroni-corrected  p=0.77), 95% CI [-0.53,0.20]) also were not  statistically significant. Figure 1 shows the point  estimate and confidence interval for each effect. 
1743-flu-shots-va-new-york-harbor.txt:getting the flu shot was about 49 days, and there  was no statistically significant difference between  the groups (p = .93, 95% CI = [–2.39, 2.18]).  demonstrates the feasibility of  rapidly testing a flu shot promotion  effort, with results available in time  to inform efforts in the next flu  season. This particular email  outreach effort was not effective in promoting  enrolled Veterans to get flu shots or to get them  earlier. There are many possible reasons for this. It  is possible, of course, that the behavioral barriers  targeted by these emails (including simple lack of  information about flu shot availability) were not  actually significant barriers for Veterans enrolled  with the New York Harbor VA Health Care System.  However, operational data revealed an important  practical limitation on any impact the email might  have had: Each of the three emails was opened by  only about 20%–25% of intended recipients.  Future work might be directed at discovering the  reasons for these low open rates and possible ways  of increasing them. 
1743-flu-shots-va-new-york-harbor.txt:effect of the emails on either  uptake or timing of flu shots. In  both groups, flu shot uptake was  about 20%. After a pre-specified  adjustment for individual  characteristics including age, rurality, and prior flu  shots, the difference between the email and  no-email groups was not statistically significant (p =  .36, 95% CI = [–.005, .013]), and the precision in this  estimate rules out an effect as small as 0.9  percentage points. The two groups were also  similar in timing. For those who got flu shots in both  groups, the average days elapsed from the start of  flu vaccine season (October 1) to an individual’s 
